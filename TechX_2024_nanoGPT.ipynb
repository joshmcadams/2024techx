{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObe1dv/RpDBz+pA98z0u9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshmcadams/2024techx/blob/main/TechX_2024_nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Hello\n",
        "\n",
        "We are printing hello below."
      ],
      "metadata": {
        "id": "CcwILlcmntbZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUbIjRgbmnMB",
        "outputId": "de2c2363-4295-4355-a835-db86e25d2939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Tech Exchange!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello Tech Exchange!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the data"
      ],
      "metadata": {
        "id": "GtHttiAUoO58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://gir.fyi/techx/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt-XY2KfoQUD",
        "outputId": "8b10fcf3-e542-4e6e-f71f-cdad23e733d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 15:24:28--  http://gir.fyi/techx/input.txt\n",
            "Resolving gir.fyi (gir.fyi)... 216.239.38.21, 216.239.36.21, 216.239.34.21, ...\n",
            "Connecting to gir.fyi (gir.fyi)|216.239.38.21|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt [following]\n",
            "--2024-05-16 15:24:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-16 15:24:28 (98.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_training_data = f.read()\n",
        "\n",
        "print(len(raw_training_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5IGnnyouGy",
        "outputId": "503e7658-01c3-47e0-db14-37fd9b658490"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(raw_training_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udvH58qRpMmn",
        "outputId": "6378751b-7cb9-4688-d23a-d8bb9b16fe08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(sorted(set(raw_training_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wmQyecNVpXxd",
        "outputId": "46498f2c-06f2-4ed9-edab-5da8422e6f52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "freqs = defaultdict(int)\n",
        "\n",
        "for letter in raw_training_data:\n",
        "  freqs[letter] += 1\n",
        "\n",
        "freqs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgfkGRdlp1uk",
        "outputId": "11a513bb-e816-40e2-dc04-b11876e090bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'F': 1797,\n",
              "             'i': 45537,\n",
              "             'r': 48889,\n",
              "             's': 49696,\n",
              "             't': 67009,\n",
              "             ' ': 169892,\n",
              "             'C': 3820,\n",
              "             'z': 356,\n",
              "             'e': 94611,\n",
              "             'n': 48529,\n",
              "             ':': 10316,\n",
              "             '\\n': 40000,\n",
              "             'B': 2761,\n",
              "             'f': 15770,\n",
              "             'o': 65798,\n",
              "             'w': 17585,\n",
              "             'p': 10808,\n",
              "             'c': 15623,\n",
              "             'd': 31358,\n",
              "             'a': 55507,\n",
              "             'y': 20448,\n",
              "             'u': 26584,\n",
              "             'h': 51310,\n",
              "             ',': 19846,\n",
              "             'm': 22243,\n",
              "             'k': 7088,\n",
              "             '.': 7885,\n",
              "             'A': 7819,\n",
              "             'l': 33339,\n",
              "             'S': 4523,\n",
              "             'Y': 1718,\n",
              "             'v': 7793,\n",
              "             '?': 2462,\n",
              "             'R': 4869,\n",
              "             'M': 2840,\n",
              "             'W': 3530,\n",
              "             \"'\": 6187,\n",
              "             'L': 3876,\n",
              "             'I': 11832,\n",
              "             'N': 5079,\n",
              "             'g': 13356,\n",
              "             ';': 3628,\n",
              "             'b': 11321,\n",
              "             '!': 2172,\n",
              "             'O': 5481,\n",
              "             'j': 628,\n",
              "             'V': 798,\n",
              "             '-': 1897,\n",
              "             'T': 7015,\n",
              "             'H': 3068,\n",
              "             'E': 6041,\n",
              "             'U': 3313,\n",
              "             'D': 2089,\n",
              "             'P': 1641,\n",
              "             'q': 609,\n",
              "             'x': 529,\n",
              "             'J': 320,\n",
              "             'G': 2399,\n",
              "             'K': 1584,\n",
              "             'Q': 231,\n",
              "             '&': 3,\n",
              "             'Z': 198,\n",
              "             'X': 112,\n",
              "             '3': 27,\n",
              "             '$': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "SAMPLE_SIZE = 500\n",
        "training_data_size = len(raw_training_data)\n",
        "start = random.randrange(0, training_data_size - SAMPLE_SIZE)\n",
        "print(raw_training_data[start:start+SAMPLE_SIZE])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_fatCHqqihm",
        "outputId": "e4295d66-250b-4fae-cc10-8103db951e7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Providence divine.\n",
            "Some food we had and some fresh water that\n",
            "A noble Neapolitan, Gonzalo,\n",
            "Out of his charity, being then appointed\n",
            "Master of this design, did give us, with\n",
            "Rich garments, linens, stuffs and necessaries,\n",
            "Which since have steaded much; so, of his gentleness,\n",
            "Knowing I loved my books, he furnish'd me\n",
            "From mine own library with volumes that\n",
            "I prize above my dukedom.\n",
            "\n",
            "MIRANDA:\n",
            "Would I might\n",
            "But ever see that man!\n",
            "\n",
            "PROSPERO:\n",
            "Now I arise:\n",
            "Sit still, and hear the last of our sea-sorrow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "yMW8rYWbsE_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = ''.join(sorted(set(raw_training_data)))\n",
        "token_to_number = {t: i for i, t in enumerate(tokens)}\n",
        "token_to_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJDOOxaQsHY8",
        "outputId": "0103fa22-4162-4b2a-e890-31d1f3a469b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_to_token = {i: t for i, t in enumerate(tokens)}\n",
        "number_to_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31nN5H8Gs-DZ",
        "outputId": "dd5b3254-b244-41d5-d312-d53bdf298269"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '$',\n",
              " 4: '&',\n",
              " 5: \"'\",\n",
              " 6: ',',\n",
              " 7: '-',\n",
              " 8: '.',\n",
              " 9: '3',\n",
              " 10: ':',\n",
              " 11: ';',\n",
              " 12: '?',\n",
              " 13: 'A',\n",
              " 14: 'B',\n",
              " 15: 'C',\n",
              " 16: 'D',\n",
              " 17: 'E',\n",
              " 18: 'F',\n",
              " 19: 'G',\n",
              " 20: 'H',\n",
              " 21: 'I',\n",
              " 22: 'J',\n",
              " 23: 'K',\n",
              " 24: 'L',\n",
              " 25: 'M',\n",
              " 26: 'N',\n",
              " 27: 'O',\n",
              " 28: 'P',\n",
              " 29: 'Q',\n",
              " 30: 'R',\n",
              " 31: 'S',\n",
              " 32: 'T',\n",
              " 33: 'U',\n",
              " 34: 'V',\n",
              " 35: 'W',\n",
              " 36: 'X',\n",
              " 37: 'Y',\n",
              " 38: 'Z',\n",
              " 39: 'a',\n",
              " 40: 'b',\n",
              " 41: 'c',\n",
              " 42: 'd',\n",
              " 43: 'e',\n",
              " 44: 'f',\n",
              " 45: 'g',\n",
              " 46: 'h',\n",
              " 47: 'i',\n",
              " 48: 'j',\n",
              " 49: 'k',\n",
              " 50: 'l',\n",
              " 51: 'm',\n",
              " 52: 'n',\n",
              " 53: 'o',\n",
              " 54: 'p',\n",
              " 55: 'q',\n",
              " 56: 'r',\n",
              " 57: 's',\n",
              " 58: 't',\n",
              " 59: 'u',\n",
              " 60: 'v',\n",
              " 61: 'w',\n",
              " 62: 'x',\n",
              " 63: 'y',\n",
              " 64: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(s):\n",
        "  return_value = []\n",
        "  for character in s:\n",
        "    return_value.append(token_to_number[character])\n",
        "  return return_value\n",
        "\n",
        "encode(\"Tech Exchange\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2u0kZS7tQtY",
        "outputId": "475344c5-3c9d-41b9-f5be-bc8ec8a929fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 43, 41, 46, 1, 17, 62, 41, 46, 39, 52, 45, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(nums):\n",
        "  return ''.join([number_to_token[n] for n in nums])\n",
        "\n",
        "decode(encode(\"Tech Exchange\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uF5iSYiSt5gq",
        "outputId": "69e73e15-1949-4760-f055-930941b25f36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tech Exchange'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# data = torch.tensor(encode(raw_training_data), dtype=torch.long)\n",
        "\n",
        "data = torch.tensor(encode(raw_training_data), dtype=torch.long)"
      ],
      "metadata": {
        "id": "4duZihoAu0v3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_size = int(0.1 * len(data))\n",
        "holdout_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnHjgTC_voj8",
        "outputId": "1556f997-efea-45c9-c637-692d04a44c71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111539"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data[:holdout_size]\n",
        "training_data = data[holdout_size:]\n",
        "len(test_data), len(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-dJu-C9wPrq",
        "outputId": "40ca7ca2-725b-46a6-be27-ea294daf9cd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(111539, 1003855)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 8\n",
        "training_data[:BLOCK_SIZE+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSOkoXpVxSfL",
        "outputId": "7bb01433-c562-481a-9c1f-60d7a5565f98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 58,  6,  1, 25, 39, 56, 41, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(3434)\n",
        "start_from = random.randint(0, training_data_size-BLOCK_SIZE)\n",
        "decode(training_data[start_from:start_from+BLOCK_SIZE].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nDFd_vIXyGq_",
        "outputId": "1caf4f38-3944-4c08-9533-d4c11d82c2b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'like asi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = training_data[:BLOCK_SIZE]\n",
        "y = training_data[1:BLOCK_SIZE+1]\n",
        "for t in range(BLOCK_SIZE):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"in: {context}, out: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZXarl4Kyun9",
        "outputId": "1e97ad3a-947d-4b6b-c1cc-a50b55f14d60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: tensor([43]), out: 58\n",
            "in: tensor([43, 58]), out: 6\n",
            "in: tensor([43, 58,  6]), out: 1\n",
            "in: tensor([43, 58,  6,  1]), out: 25\n",
            "in: tensor([43, 58,  6,  1, 25]), out: 39\n",
            "in: tensor([43, 58,  6,  1, 25, 39]), out: 56\n",
            "in: tensor([43, 58,  6,  1, 25, 39, 56]), out: 41\n",
            "in: tensor([43, 58,  6,  1, 25, 39, 56, 41]), out: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "l5BEJs7603og"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = training_data if split == 'train' else test_data\n",
        "  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  #print(ix)\n",
        "  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "get_batch('train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYQaibfl1Bx_",
        "outputId": "9e35f6cc-5a6c-4302-a12f-a668ebe3e6ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[42,  1, 58, 59, 56, 52, 57,  1],\n",
              "         [ 1, 17, 52, 45, 50, 39, 52, 42],\n",
              "         [47, 58, 46,  1, 46, 43, 56,  1],\n",
              "         [ 1, 46, 47, 57,  1, 44, 43, 50]]),\n",
              " tensor([[ 1, 58, 59, 56, 52, 57,  1, 58],\n",
              "         [17, 52, 45, 50, 39, 52, 42,  5],\n",
              "         [58, 46,  1, 46, 43, 56,  1, 58],\n",
              "         [46, 47, 57,  1, 44, 43, 50, 50]]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "\n",
        "for b in range(BATCH_SIZE):\n",
        "  for t in range(BLOCK_SIZE):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'input {context.tolist()}, target {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tbh3RgD3lEj",
        "outputId": "8b3ed5ca-72e9-42b3-f671-a80defb3f2d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input [50], target 47\n",
            "input [50, 47], target 60\n",
            "input [50, 47, 60], target 43\n",
            "input [50, 47, 60, 43], target 42\n",
            "input [50, 47, 60, 43, 42], target 1\n",
            "input [50, 47, 60, 43, 42, 1], target 57\n",
            "input [50, 47, 60, 43, 42, 1, 57], target 53\n",
            "input [50, 47, 60, 43, 42, 1, 57, 53], target 1\n",
            "input [45], target 53\n",
            "input [45, 53], target 53\n",
            "input [45, 53, 53], target 42\n",
            "input [45, 53, 53, 42], target 1\n",
            "input [45, 53, 53, 42, 1], target 39\n",
            "input [45, 53, 53, 42, 1, 39], target 1\n",
            "input [45, 53, 53, 42, 1, 39, 1], target 45\n",
            "input [45, 53, 53, 42, 1, 39, 1, 45], target 47\n",
            "input [47], target 45\n",
            "input [47, 45], target 46\n",
            "input [47, 45, 46], target 58\n",
            "input [47, 45, 46, 58], target 11\n",
            "input [47, 45, 46, 58, 11], target 0\n",
            "input [47, 45, 46, 58, 11, 0], target 14\n",
            "input [47, 45, 46, 58, 11, 0, 14], target 59\n",
            "input [47, 45, 46, 58, 11, 0, 14, 59], target 58\n",
            "input [52], target 1\n",
            "input [52, 1], target 58\n",
            "input [52, 1, 58], target 46\n",
            "input [52, 1, 58, 46], target 63\n",
            "input [52, 1, 58, 46, 63], target 1\n",
            "input [52, 1, 58, 46, 63, 1], target 44\n",
            "input [52, 1, 58, 46, 63, 1, 44], target 39\n",
            "input [52, 1, 58, 46, 63, 1, 44, 39], target 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "i52EW73n4Uga"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 8\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 300\n",
        "EVAL_ITERS = 200\n",
        "NUM_EMBEDDINGS = 32\n"
      ],
      "metadata": {
        "id": "S8-HRMGHLe7Q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(raw_training_data))\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.query = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.value = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(NUM_EMBEDDINGS, NUM_EMBEDDINGS)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, NUM_EMBEDDINGS)\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, NUM_EMBEDDINGS)\n",
        "    #self.sa_head = Head(NUM_EMBEDDINGS)\n",
        "    self.feed_forward = FeedForward(NUM_EMBEDDINGS)\n",
        "    self.sa_heads = MultiHeadAttention(4, NUM_EMBEDDINGS//4)\n",
        "    self.lm_head = nn.Linear(NUM_EMBEDDINGS, vocab_size)\n",
        "    self.blocks = nn.Sequential(\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      nn.LayerNorm(NUM_EMBEDDINGS),\n",
        "    )\n",
        "    self.proj = nn.Linear(NUM_EMBEDDINGS, NUM_EMBEDDINGS)\n",
        "\n",
        "  def forward(self, token, targets=None):\n",
        "    token_embeddings = self.token_embedding_table(token)\n",
        "    B, T = token.shape\n",
        "    position_embeddings = self.position_embedding_table(torch.arange(T))\n",
        "    x = token_embeddings + position_embeddings\n",
        "\n",
        "\n",
        "    x = token_embeddings + position_embeddings\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "print(decode(m.generate(z, 40)[0].tolist()))"
      ],
      "metadata": {
        "id": "CqpJkFnf4ZlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ce4271-0be3-43b2-8bc6-26d90a2af0f8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OyaJ-lq?fdojPP;AtsvJcz&:zZIdVTJk!T-OlySp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(EVAL_ITERS)\n",
        "    for k in range(EVAL_ITERS):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "nBfblENXMOg4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIxpnJAIHxyu",
        "outputId": "a5a76cf2-7d81-494b-83bc-fcedde5cfbb7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.1951; val loss 4.1948\n",
            "iter 300; train loss 2.9104; val loss 2.9062\n",
            "iter 600; train loss 2.6137; val loss 2.6222\n",
            "iter 900; train loss 2.5297; val loss 2.5561\n",
            "iter 1200; train loss 2.4947; val loss 2.4977\n",
            "iter 1500; train loss 2.4801; val loss 2.4913\n",
            "iter 1800; train loss 2.4664; val loss 2.4772\n",
            "iter 2100; train loss 2.4318; val loss 2.4739\n",
            "iter 2400; train loss 2.4214; val loss 2.4575\n",
            "iter 2700; train loss 2.4216; val loss 2.4357\n",
            "iter 3000; train loss 2.4161; val loss 2.4443\n",
            "iter 3300; train loss 2.4143; val loss 2.4265\n",
            "iter 3600; train loss 2.3977; val loss 2.4211\n",
            "iter 3900; train loss 2.3991; val loss 2.4204\n",
            "iter 4200; train loss 2.3888; val loss 2.4174\n",
            "iter 4500; train loss 2.3840; val loss 2.4231\n",
            "iter 4800; train loss 2.3797; val loss 2.4153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "generated = m.generate(z, max_new_tokens=100)\n",
        "generated_list = generated[0].tolist()\n",
        "print(decode(generated_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBl5bMXFKKf2",
        "outputId": "ac4b261e-7717-47dc-9b47-e39e6d467b94"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GO sepor he.\n",
            "\n",
            "Thes ware his ty ot as, he?\n",
            "Ashe, sthetry,\n",
            "I'ioe\n",
            "siotefiur ht. Che ds pres the tevee n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKrRj3LLPx_m",
        "outputId": "b5d49d9a-d543-4000-c9b5-d24d4bad0e2d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(sorted(set(raw_training_data)))[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5UyaoQ7QC0T_",
        "outputId": "3f36bb5c-6170-47aa-f26e-ebead80edc8c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "-np.log(1/65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SL8und6B7r0",
        "outputId": "7ee3407d-35a1-4284-d91c-d77ceff3fd6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.174387269895637"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Version"
      ],
      "metadata": {
        "id": "k3sZrVcn9GsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "        nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "JOUkYfx69Eei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}